import re
import requests
import time
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from pathlib import Path
from typing import List, Optional
from dataclasses import dataclass, field

OUTPUT_FILE = "DocumentationUCZONE.md"
URLS_FILE = "urls.txt"

def strip_gitbook_noise(markdown_text: str) -> str:
    """–ß–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π GitBook/markdownify –∏–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è—é—Ç –≤ MD."""
    if not markdown_text:
        return markdown_text

    lines = markdown_text.splitlines()
    out: List[str] = []

    noise_patterns = [
        re.compile(r"^\s*copy\s*$", re.IGNORECASE),
        re.compile(r"^\s*{%\s*hint\s+style=\"info\"\s*%}\s*$"),
        re.compile(r"^\s*{%\s*endhint\s*%}\s*$"),
    ]

    for line in lines:
        s = line.strip()
        if s and any(p.search(s) for p in noise_patterns):
            continue
        out.append(line)

    cleaned = "\n".join(out)
    cleaned = cleaned.replace('{% hint style="info" %}', "")
    cleaned = cleaned.replace("{% endhint %}", "")
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)
    return cleaned.strip()

@dataclass
class PageGroup:
    """–ì—Ä—É–ø–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–±—â–µ–π —Ç–µ–º–æ–π"""
    title: str
    match_pattern: str  # –ü–æ–¥—Å—Ç—Ä–æ–∫–∞ URL, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∞—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≥—Ä—É–ø–ø–µ
    level: int  # –£—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (1 = –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–∞–∑–¥–µ–ª, 2 = –ø–æ–¥—Ä–∞–∑–¥–µ–ª)
    urls: List[str] = field(default_factory=list)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
# –ü–æ—Ä—è–¥–æ–∫ –≤ —Å–ø–∏—Å–∫–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –≤ –∏—Ç–æ–≥–æ–≤–æ–º —Ñ–∞–π–ª–µ.
# URL –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –≥—Ä—É–ø–ø—É —Å —Å–∞–º—ã–º –¥–ª–∏–Ω–Ω—ã–º —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º match_pattern.
STRUCTURE_DEFINITIONS = [
    PageGroup("Starting Guide", "api-v2.0", 1), # Fallback for root
    
    # Cheats Types
    PageGroup("Cheats Types and Callbacks", "cheats-types-and-callbacks", 1),
    PageGroup("Classes - Color", "cheats-types-and-callbacks/classes/color", 2),
    PageGroup("Classes - Menu System", "cheats-types-and-callbacks/classes/menu", 2),
    PageGroup("Classes - UI Widgets", "cheats-types-and-callbacks/classes/widgets", 2),
    PageGroup("Classes - Math", "cheats-types-and-callbacks/classes/math", 2),
    
    # Game Components
    PageGroup("Game Components - Entity Lists", "game-components/lists", 1),
    PageGroup("Game Components - Core Objects", "game-components/core", 1),
    PageGroup("Game Engine", "game-components/game-engine", 1),
    PageGroup("Networking and APIs", "game-components/networking-and-apis", 1),
    
    # Rendering
    PageGroup("Rendering and Visuals", "game-components/rendering-and-visuals", 1),
    PageGroup("Rendering - Panorama UI", "game-components/rendering-and-visuals/panorama", 2),
    
    # Config
    PageGroup("Configuration and Utilities", "game-components/configuration-and-utilities", 1),
]

def load_urls_from_file(filepath: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ñ–∞–π–ª–∞."""
    path = Path(filepath)
    if not path.exists():
        print(f"‚ö†Ô∏è –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return []
    
    with open(path, "r", encoding="utf-8") as f:
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]

def distribute_urls(urls: List[str], groups: List[PageGroup]):
    """–†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç URL –ø–æ –≥—Ä—É–ø–ø–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ match_pattern."""
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã –ø–æ –¥–ª–∏–Ω–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    # –ù–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
    
    for url in urls:
        best_match_group = None
        max_len = -1
        
        for group in groups:
            if group.match_pattern in url:
                if len(group.match_pattern) > max_len:
                    max_len = len(group.match_pattern)
                    best_match_group = group
        
        if best_match_group:
            best_match_group.urls.append(url)
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –ø–µ—Ä–≤—É—é –≥—Ä—É–ø–ø—É (Starting Guide) –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º Misc
            print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –≥—Ä—É–ø–ø–∞ –¥–ª—è URL: {url}")
            groups[0].urls.append(url)

def get_markdown_from_html(url: str) -> Optional[str]:
    """Fallback-–º–µ—Ç–æ–¥: –ø–∞—Ä—Å–∏—Ç HTML –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ Markdown."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        main_content = soup.find('main')
        if not main_content:
            return None

        for element in main_content.find_all(['nav', 'footer']):
            element.decompose()
        for link in main_content.find_all('a', class_=lambda x: x and 'pagination' in str(x)):
            link.decompose()

        markdown_text = md(str(main_content), heading_style="ATX")
        return strip_gitbook_noise(markdown_text)
    except Exception as e:
        print(f"   ‚ùå Fallback –æ—à–∏–±–∫–∞: {e}")
        return None

def get_markdown_content(url: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —á–∏—Å—Ç—ã–π Markdown –Ω–∞–ø—Ä—è–º—É—é –∏–∑ GitBook."""
    try:
        markdown_url = url if url.endswith('.md') else f"{url}.md"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        
        response = requests.get(markdown_url, headers=headers, timeout=15)
        response.raise_for_status()
        content = response.text.strip()

        if content.startswith('<!DOCTYPE') or content.startswith('<html'):
            print(f"   üîÑ .md –≤–µ—Ä–Ω—É–ª HTML, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
            return get_markdown_from_html(url)

        return strip_gitbook_noise(content)
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            print(f"   üîÑ .md –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (404), –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
            return get_markdown_from_html(url)
        return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
        return get_markdown_from_html(url)

def generate_toc(groups: List[PageGroup]) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ."""
    toc_lines = ["# –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ\n"]
    for group in groups:
        if not group.urls: continue
        indent = "  " * (group.level - 1)
        anchor = group.title.lower().replace(" ", "-").replace("_", "-")
        toc_lines.append(f"{indent}- [{group.title}](#{anchor})")
    return "\n".join(toc_lines) + "\n\n"

def main():
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ URL
    urls = load_urls_from_file(URLS_FILE)
    if not urls:
        print("‚ùå –ù–µ—Ç URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥—Ä—É–ø–ø–∞–º
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –≥—Ä—É–ø–ø, —á—Ç–æ–±—ã –æ—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏
    active_groups = [g for g in STRUCTURE_DEFINITIONS] 
    for g in active_groups: g.urls = []
    
    distribute_urls(urls, active_groups)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –≥—Ä—É–ø–ø—ã
    active_groups = [g for g in active_groups if g.urls]

    total_pages = sum(len(group.urls) for group in active_groups)
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ {len(active_groups)} –≥—Ä—É–ø–ø...")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("# UCZONE API v2.0 - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n\n")
        f.write(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü GitBook*\n\n")
        f.write("---\n\n")
        f.write(generate_toc(active_groups))
        f.write("\n" + "=" * 80 + "\n\n")

        processed = 0
        success_count = 0

        for group_idx, group in enumerate(active_groups, 1):
            heading_level = "#" * (group.level + 1)
            f.write(f"{heading_level} {group.title}\n\n")

            for url_idx, url in enumerate(group.urls, 1):
                processed += 1
                print(f"[{processed}/{total_pages}] {group.title} ({url_idx}/{len(group.urls)}): {url}")
                
                content = get_markdown_content(url)
                
                if content:
                    f.write(f"<!-- Source: {url} -->\n\n")
                    f.write(content)
                    f.write("\n\n")
                    success_count += 1
                    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                else:
                    f.write(f"> ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}\n\n")
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")

                time.sleep(0.3)

            if group_idx < len(active_groups):
                f.write("\n" + "-" * 80 + "\n\n")

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()